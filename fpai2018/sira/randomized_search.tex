\section{提案手法}
既存手法のgBoostでは探索木上を深さ優先的に厳密探索する。
しかし、入力グラフ集合に対して、個々のグラフの大きさや全体数の増加に伴い、
存在しうる部分グラフの総数は急激に増加するため、厳密探索におけるコストは莫大なものである。
問題によってモデル構築が困難なものも存在する。
本稿では、厳密探索を行わず、乱数を用いて探索をランダム化する手法を提案する。

まず初めに、本提案手法で用いる$flow$の概念を説明する。\\
1.ある定数$k$を定める。\\
2.探索木の根ノードから探索を開始する。\\
3.現探索ノードにおける子ノードを列挙する。\\
4.列挙した$m$個の各子ノード$i$に対して、確率$p(i) (1 \leq i \leq m)$を定める。\\
5.定めた確率に基づいて、入力定数$k$を各子ノードに割り振る。\\
6.各子ノードにおいて、割り振られた数を入力定数とし3〜5を再帰的に行う。\\
7.探索が末端ノードに到達した場合、入力定数が1以上であっても再帰を終了する。
(ここで末端ノードとは子ノードを持たないノード、枝刈り可能なノード、指定したグラフの大きさを上回る部分グラフを表現するノードのことを指す。)

以上の探索を$k\ flow$探索と定義する。
Algorithm\ref{alg_main}に擬似コードを示す。
%アルゴリズムがここに入る
\begin{algorithm}
  \small 
\caption{探索のランダム化}\label{alg_main}
\begin{algorithmic}[1]
\Procedure{最適な部分グラフ、あるいはその共起}{}
\State グローバル変数 : $g^*,\omega^*,p^*,pc^*$
\State 最適な部分グラフ$g^*,\omega^*,p^*$とTop-k部分グラフを探索 \Comment{gBoost法の探索}
\ForAll{$p \in $ Top-k 部分グラフ}
\State project(p)
\EndFor
\EndProcedure

\Function{tmp}{$p,t$}
\If {$b(t) < g^*$}
\State \textbf{return}
\EndIf
\EndFunction
\end{algorithmic}
\end{algorithm}

%アルゴリズムの説明がここに入る

本研究では探索手順における、子ノードの選択確率$p_i$に関して、以下に示す3つの確率設定手法を提案する。

\subsection{提案手法1：一様確率設定}
提案手法1では、子ノードに対して一様に確率を設定する。
現探索ノードにおける$i$番目の子ノードの選択確率を以下に示す。

\begin{equation}
	p(i) = 1 / m.
\end{equation}

\subsection{提案手法2：支持度(support)の情報を加えた確率設定}
提案手法1は、入力グラフの特徴を利用しないナイーブな手法である。
本節では、入力グラフに対する支持度(support)の情報を利用した手法を提案する。
支持度とは、ある部分グラフが全入力グラフの内いくつのグラフで現れるかを表す値であり以下の式で定義される。

\begin{equation}
	support(x) = \sum_{n=1}^{l} \mathbb{I} (x \subseteq G_n) (x \in \mathcal{T}).
\end{equation}

探索木の性質上、支持度が大きい部分グラフのほうが小さいものに比べて
複数の入力グラフ上での拡大グラフを考えることができるため、子孫空間が広いことを期待できる。
子孫空間が狭いノードに多くの$flow$を流す場合、探索が広がらず無駄になる可能性が大きい。
したがって、本手法では支持度が大きい子ノードが選択されやすくなるよう確率を定める。
$i$番目の子ノードが表す部分グラフを$x_i$とすると、子ノードの選択確率を以下の式で示す。

\begin{equation}
	p(i) = support(x_i) / \sum_{n=1}^{m} support(x_n).
\end{equation}

\subsection{提案手法3：bound,gainの情報を加えた確率設定}
本節では、gainとboundの情報を利用した手法を提案する。
gain、boundは、本稿第2章で定義したものを用いる。
boundは子孫ノードで得ることのできるgainの上限値を表すため、
boundの大きな子ノードへの探索を行うことで、より大きなgainの値を得ることが期待できる。
また、gainの値はboundの値との探索空間上の距離を計る指標になることが期待できるため、
gainとboundの凸結合の値を用いて子の選択確率を定めることで、より効率的な探索を行う。
子ノードの選択確率を以下の式で示す。

\begin{eqnarray}
	p(i) & = & val(x_i) / \sum_{n=1}^{m} val(x_n), \\
	val(x_i) & = & \lambda gain(x_i) + ( 1 - \lambda ) bound(x_i). \nonumber 
\end{eqnarray}

$\lambda$はgainとboundの重みを表すパラメータであり、$0 \leq \lambda \leq 1$を満たす。

\subsection{探索空間の同型判定除外}
既存手法で用いられるDFSコード木はgSpan\cite{gSpan}を用いて構築される木である。
gSpanアルゴリズムでは、グラフ同型問題を解き、
同型である場合には当アルゴリズムで定義される辞書順を用いて最小のものだけを木に採用する。
当アルゴリズムにおいて辞書順最小のものは木全体に対して左側に出現しやすいため、
DFSコード木は木全体の形が左寄りになるという特徴を持つ。
ここで問題となるのが、探索木が左寄りである時、提案手法1のように一様に確率を設定する場合においても、
同階層のノードへの探索確率に大きな差が生じてしまうということである。理解を簡単にするため、以下に図を示す。

図

本研究では、gSpanアルゴリズムでのグラフ同型判定を行わずに、
辞書順最小でないノードも木に追加することで、
木全体の偏りを緩和し、以上のような探索確率の差が軽減するよう試みる。
提案手法2、3においても、同一の探索木を利用する。


\section{実験\&結果}
提案手法を用いて構築されるモデルの評価を行うために、3つの実験を行う。

\subsection{データセット}
\begin{table}
	\centering
	\scalebox{0.75}{
		\begin{tabular}{|c|r|r|r|r|r|}
			\hline
			データ名 & グラフ数 & 平均ノード数 & 平均エッジ数 & 正例　& 負例\\
			\hline \hline
			CPDB  & 684 & 25.2 & 25.6 & 341 & 343\\
			\hline
			Mutag & 188 & 26.3 & 28.1 & 125 & 63\\
			\hline
		\end{tabular}
	}
	\caption{使用したデータセット}
	\label{dataset}
\end{table}

本稿では、Saigoら\cite{gBoost}によって用いられた2つのデータセットを使用した。
表\ref{dataset}に各データセットのグラフ数、平均ノード・エッジ数、正例と負例の数を示す。

\subsection{実験1：探索の精度、効率に関する実験}
%%%%探索に関する精度(どれだけ探索出来てるか）を計る実験　＜　目的関数の値で評価
実験1では、CPDBとMutagのデータに対して各提案手法がどれだけ精度よく探索できているか、
どれだけ既存手法より効率化できているかを計る実験を行う。
本稿では、主問題における目的関数(2章(5)式)の値を探索精度の指標に、
モデル構築までにgainとboundの値を計算したノードの総数を探索ノード数とし、効率化の指標に用いる。

以下では、誤分類に対するコスト制御のパラメータ$\nu$の値が0.1、0.3、0.5の3つの場合に対して実験を行う。

\subsubsection{flow - 目的関数}
$flow$の数を変化させた時の目的関数の最終値の推移を調べる。
$flow$の刻み幅は、提案手法1、2に対しては10とする。
提案手法3は、アルゴリズム上、子ノードのgainとboundを全て計算する必要があるため、
提案手法1，2と比べて探索ノード数が多くなる。したがって、$flow$の刻み幅は1とする。

\subsubsection{結果}

\subsubsection{flow - 探索ノード数}
$flow$の数を変化させた時の探索ノード数の推移を調べる。
$flow$の刻み幅は、前述したものと同じ値とする。

\subsubsection{結果}

\subsubsection{探索ノード数 - 目的関数}
各提案手法の探索ノード数に対して、目的関数の最終値の推移を調べる。

\subsubsection{結果}

\subsection{実験2：$\lambda$ に関する実験}
実験2では、提案手法3におけるgainとboundの重みパラメータ$\lambda$の値を変化させた時の目的関数の最終値、
探索ノード数の推移を調べる。
$flow$数を100に固定し、$\lambda$の値を0から1まで0.1ずつ変動させる。
これをデータに対し10 fold cross varidationを行う。 
以上の実験を実験1と同様に、$\nu$の値が0.1、0.3、0.5の場合に対して行う。

\subsection{実験3：乱数のブレに関する実験}
提案手法では、乱数を使用しているため、結果にブレが生じる。
したがって、実験3では各提案手法が乱数にどれだけ影響を受けるのか、そのブレを調べる。
$flow$数を100に固定し、乱数に10個のシード値を与えた時の目的関数の最終値、探索ノード数の平均、分散値を調べる。
以上の実験を実験1と同様に、$\nu$の値が0.1、0.3、0.5の場合に対して行う。

