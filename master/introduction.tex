\section{Introduction}
Graphs are fundamental data structures for representing combinatorial objects 
such as chemical compounds, networks and others.
However, precisely because of their combinatorial nature, 
it is usually difficult to understand the underlying trends in large datasets of graphs.
The rapid increase in data in recent years also includes data
represented as graphs, and thus supervised learning in which the inputs are graphs of
arbitrary size and shape has gained considerable attention
in the fields of computer vision \cite{Harchaoui:2007, Nowozin:2007, Barra:2013, Bai:2014a},
chemoinformatics \cite{Kashima:2003, Tsuda:2007,Saigo:2008a, Saigo:2009,Mahe:2009, 
Vishwanathan:2010, Shrvashidze:2011, Takigawa:2013, takigawa:2017},
bioinformatics\cite{Borgwardt:2005, Karklin:2005, Takigawa:2011b} 
and computational chemistry\cite{Kearnes2016, gilmer:2017} 
and natural language processing\cite{Kudo:2005}. 

Some of these problems have graph descriptors prepared in advance, 
such as molecular dataset\cite{james:2004, durant:2002}, 
but others have no direct descriptors available.
For these problems, the most fundamental features are subgraph indicators, 
i.e., if a subgraph is embedded in the given graph or not,
because many reactions and events are attributed to those substructures.
However, the number of all possible subgraph patterns is intractably large 
due to the combinatorial explosion, 
so that it is difficult to enumerate all subgraphs in a realistic time for a given dataset.
Therefore, it is necessary to restrict the subgraph candidate on the basis of some criterion.
Frequent subgraph mining \cite{Yan:2002, Nijssen:2004} is one method in this approach.
However, since it chooses subgraphs according to the frequencies, 
it is possible to overlook the importance of subgraphs.
On the other hand, discriminative subgraph mining 
\cite{Yan:2008, Fan:2008, Saigo:2009, Shirakawa:2018} 
choose subgraphs according to some discriminative criteria, 
and not likely to overlook important subgraphs for learning.
Because of this merit, the present paper also adopts this approach.

\subsection{Related Work and Our Motivation}
\label{sec:relatedwork}
Frequent subgraph mining methods, such as gSpan \cite{Yan:2002} or Gaston \cite{Nijssen:2004}, 
are one of the solution of graph classification/regression with subgraph features.
These methods make feature vector by enumerating frequently appearing subgraph, 
and an existing machine learning model is applied to the vector.
This learning method is called two step approach, 
because the mining part and learning part are completely separated.
However Wale et al. \cite{Wale:2008} demonstrated experimentally that 
two step approach is costly in time and memory 
and it is harmful in accuracy to restrict features by frequency.

Discriminative subgraph mining methods are used instead to solve these problems.
Fan et al. \cite{Fan:2008} proposed direct mining method 
that searches a significant subgraph based on information gain and simultaneously learns decision tree.
This method is combined with frequent pattern mining and not consider all possible subgraph yet.
Saigo et al. \cite{Saigo:2009} removed frequency constraint 
by using model-based discriminative criteria 
and designing the pruning of search space using anti-monotonic of frequency.
It learned decision stump directly,  
and improved the accuracy by taking an ensemble.
Shirakawa et al. \cite{Shirakawa:2018} directly learned regression trees instead of decision stumps
to solve non-linear problems.
Yan et al. devised Leap Search \cite{Yan:2008} that uses horizontal pruning 
instead of vertical pruning as above.
This method prunes sibling nodes using the similarity of appearance locations.

The previous discriminative subgraph mining methods have focused on reducing the mining costs by pruning, 
and searched exactly using them. 
However, since exact search is very expensive, 
there are many problems that do not scale even if pruning is used.
Therefore, we consider approximate search under fixed budget constraint instead of exact search.
Although the approximate search method is inferior to the strict search in accuracy, 
it is attracting attention as a means for obtaining a quick solution.
In this problem, our motivation is to come up with a search method 
to find out how effective features are at a given cost.


\begin{comment}
The previous discriminative subgraph mining methods have focused on reducing the mining costs by pruning, 
but all of the search policies have used naive depth-first search.
One of our motivations is that smarter search policies can make mining more efficient.
In the previous methods, 
discriminative criteria and its upper or lower bounds are calculated for every subgraph,
so designing search policy based on these may be a good solution.
Another motivation is that there is no need for an exact search.
Some of the previous methods learn ensemble models, 
and in the ensemble models it is not necessary to each weak learner be so strong.
Conversely, an exact search may not only increase search costs, but also lead to over-fitting.
Therefore, in present paper, 
consider a search that can obtain a better approximate solution under fixed budget constraint.
\end{comment}
